# 컴퓨터비전 스터디 기록 📚  

## 6주차 - MNIST 이미지 분류 실습

## ✅ 학습 목표

- MNIST 데이터셋 구조 및 클래스 라벨 이해, 시각화
- 각자 구현한 모델을 공유하고 성능 비교
- loss, accuracy 등 성능 지표 기반 학습 과정 분석
- 레이어 구조 이해 및 직접 모델 설계 기반 다지기

## 1️⃣ MNIST란?

- **MNIST**: Modified National Institute of Standards and Technology database
- 손으로 쓴 숫자 (0~9) 이미지 데이터셋
- 흑백 이미지 (1채널), 28x28 사이즈
- 10개의 클래스

## 2️⃣ 분류 모델 설계 및 실험
### 🔹 기본 모델: Logistic Regression

- 단일 선형 계층으로 구성된 선형 모델  
- 간단하지만 표현력이 낮아 복잡한 패턴 학습에 한계  
- 정확도 약 **92.29%**

### 🔹 다층 퍼셉트론 (MLP)

- **은닉층 2개 (예: 128 → 64)** 구성, ReLU 활성화 함수 사용  
- 비선형성과 표현력 증가로 성능 향상  
- 정확도 약 **97.29%**  
- **과적합 가능성**이 있으며, 이에 대한 시각화를 통해 Train/Test loss 비교  
  - Train Loss는 감소, Test Loss는 정체 → **약한 오버피팅**

### 🔹 성능 개선 기법

- **은닉층 크기 증가** → 더 많은 표현력 확보  
- **Dropout** 적용 → 학습 중 일부 뉴런 제거하여 일반화 성능 향상  
- **Batch Normalization** → 학습 안정화 및 수렴 속도 향상  
- **EarlyStopping** 및 **ReduceLROnPlateau**  
  - 성능 개선이 멈추면 학습 조기 종료, 학습률 조정  


### 🔹 CNN (합성곱 신경망)

- **Conv + Pooling + FC** 구조 활용  
- 이미지에서 특징 추출 및 학습에 효과적  
- MaxPooling으로 계산량 감소 + 과적합 방지  
- **ReLU**로 비선형성 도입  
- MLP 대비 높은 정확도 및 일반화 성능  
- 일부 모델은 Keras/Torch 프레임워크에서 **99.6% 이상의 정확도** 달성  

## ✅ 분석 요약

- **Logistic Regression**은 단순하지만 정확도 한계 있음  
- **MLP**는 비선형성과 계층 구조를 통해 높은 정확도 달성  
- **CNN**은 이미지 분류에 매우 효과적이며, 특징 추출 및 일반화에 유리  
- Dropout, BatchNorm, EarlyStop 등으로 오버피팅 방지 및 성능 개선 가능  
- 다양한 모델과 실험을 통해 **모델 구조 설계 및 성능 평가 능력 향상**